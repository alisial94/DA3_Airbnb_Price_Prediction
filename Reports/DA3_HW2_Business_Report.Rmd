---
title: "Data Analysis 3 - Assignment 2 - Business Report"
subtitle: "Perice Prediction for Arbnb Apartments in Melbourne, Australia"
author: "Ali Sial"
date: "2/9/2022"
output: pdf_document
header-includes: 
  - \usepackage{float}
  - \usepackage{longtable}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#### SET UP
# It is advised to start a new session 
# CLEAR MEMORY
rm(list=ls())


# Descriptive statistics and regressions
library(tidyverse)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(cowplot)
library(modelsummary)
library(fixest)
library(dplyr)
library(naniar)
library(stargazer)
library(Hmisc)
library(skimr)
library(gridExtra)
library(ggpubr)
library(ggplot2)
library(kableExtra)
library(data.table)
library(xtable)
library(directlabels)
library(knitr)
library(rattle)
library(ggcorrplot)
library(ranger)
library(pdp)
library(gbm)

## please down load these from my GitHub, in case you don't have these or the best would be if you could clone my repository then it would work directly. 
source("codes/da_helper_function.R")
source("codes/theme_bg.R")
```



```{r, include=FALSE}
# import data -------------------------------------------------------------
## clean data from last R script uploaded to git, calling it directly from there

data <- read_csv("https://raw.githubusercontent.com/alisial94/DA3_Airbnb_Price_Prediction/main/Data/clean/melborne_prepared.csv")

source("Codes/da_helper_function.R")
source("Codes/theme_bg.R")

# where do we have missing values now?
to_filter <- sapply(data, function(x) sum(is.na(x)))
to_filter[to_filter > 0]
# No columns with missing observations


data %>%
  group_by(f_property_type, room_type) %>%
  summarise(mean_price = mean(price))



# Creating LOWESS plots to identify the association petween price and the rest of teh variables
# for (i in colnames(data)) {
 #  ggplot(data,aes_string(x=i, y="price"))+
 #  geom_smooth(method = "loess", formula = y~x, se=FALSE)+ 
 #  geom_point()
 #  ggsave(paste0("graphs/association/",i,".png"))
# }

# Removing columns based on above created graphs
#drop <- c("d_fire_pit", "d_lake_access","d_ping_pong_table", "d_private_hot_tub","d_private_outdoor_heated_pool","d_private_outdoor_pool", "d_private_pool", "f_room_type", "n_number_of_reviews","d_bikes","d_board_games","d_game_console","d_have_fitnessgym","d_have_body_soapgel","d_have_sound_system","d_hot_tub","d_lock_on_bedroom_door","d_lockbox","d_shared_outdoor_pool","n_days_since_last_review","n_minimum_nights")
#data <- data %>%
#  select(-one_of(drop))



# Grouping variables
# Basic Variables
basic_lev  <- c("f_property_type","f_neighbourhood_cleansed","n_accommodates","f_bathroom","f_bedroom","f_beds",
                "price","d_flag_bedrooms","f_minimum_nights")
reviews <- c("f_review_score_rating","n_reviews_per_month","flag_review_scores_rating",
             "flag_days_since_first_review","ln_days_since_first_review","ln_number_of_reviews")
host <- c("f_host_response_time","p_host_response_rate","p_host_acceptance_rate","d_host_greets_you",
            "d_host_is_superhost","d_host_identity_verified","flag_host_acceptance_rate",
            "flag_host_response_rate","flag_host_response_time")
ammenities <- c("d_bath_tub","d_building_staff","d_carbon_monoxide_alarm","d_cleaning_products","d_dining_table",
                "d_drying_rack_for_clothing","d_elevator","d_essentials","d_extra_pillows_and_blankets",
                "d_fire_extinguisher","d_have_first_aid","d_hangers","d_hot_water","d_hot_water_kettle",
                "d_laundromat_nearby","d_microwave","d_outdoor_dining_area","d_outdoor_furniture",
                "d_private_entrance","d_roomdarkening_shades","d_smart_lock","d_security_cameras_on_property",
                "d_have_pool","d_smoke_alarm","d_have_kitchen","d_have_stove","d_have_oven","d_refrigerator",
                "d_coffee_machine","d_wifi","d_bbq_equipment","d_tv","d_have_iron","d_have_heating","d_cooling",
                "d_balcony","d_garden","d_have_breakfast","d_have_workspace","d_family_friendly",
                "d_luggage_dropoff_allowed","d_single_level_home","d_bathroom_essentials","d_free_parking",
                "d_paid_parking", "d_linens","d_streaming_services","d_shampoo_conditioner", "d_body_wash",
                "d_have_washer","d_have_dryer","d_clothing_storage","d_cutlary_glasses")

#df <- data
# Checking interactions
# price_diff_by_variables4 <- function(df, factor_var, dummy_var, factor_lab, dummy_lab){ 
  # Looking for interactions.
  # It is a function it takes 3 arguments: 1) Your dataframe,
  # 2) the factor variable (like room_type)
  # 3)the dummy variable you are interested in (like TV)
  
  # Process your data frame and make a new dataframe which contains the stats
 # factor_var <- as.name(factor_var)
 # dummy_var <- as.name(dummy_var)
  
 # stats <- df %>%
 #   group_by(!!factor_var, !!dummy_var) %>%
  #  dplyr::summarize(Mean = mean(price, na.rm=TRUE),
  #                   se = sd(price)/sqrt(n()))
  
 # stats[,2] <- lapply(stats[,2], factor)
  
 # ggplot(stats, aes_string(colnames(stats)[1], colnames(stats)[3], fill = colnames(stats)[2]))+
  #  geom_bar(stat='identity', position = position_dodge(width=0.9), alpha=0.8)+
  #  geom_errorbar(aes(ymin=Mean-(1.96*se),ymax=Mean+(1.96*se)),
  #                position=position_dodge(width = 0.9), width = 0.25)+
  #  scale_color_manual(name=dummy_lab,
    #                   values=c(color[2],color[1],color[3],color[4])) +
   # scale_fill_manual(name=dummy_lab,
    #                  values=c(color[2],color[1],color[3],color[4])) +
   # ylab('Mean Price')+
   # xlab(factor_lab) +
   # theme_bg()+
  #  theme(panel.grid.major=element_blank(),
  #        panel.grid.minor=element_blank(),
   #       panel.border=element_blank(),
     #     axis.line=element_line(),
    #      legend.position = "top",
          #legend.position = c(0.7, 0.9),
     #     legend.box = "vertical",
      #    legend.text = element_text(size = 5),
       #   legend.title = element_text(size = 5, face = "bold"),
        #  legend.key.size = unit(x = 0.4, units = "cm")
 #   )
#  }
# Plot interactions between property type and all dummies 
# sapply(ammenities, function(x){
#  p <- price_diff_by_variables4(df, "f_property_type", x, "property_type", x)
 # print(p)
#})



# Based on individual box plot for each amenity with property type, following will be interacted with property type
interactions <- c("f_property_type*d_bath_tub",
                  "f_property_type*d_building_staff",
                  "f_property_type*d_elevator",
                  "f_property_type*d_extra_pillows_and_blankets",
                  "f_property_type*d_fire_extinguisher",
                  "f_property_type*d_hangers",
                  "f_property_type*d_hot_water_kettle",
                  "f_property_type*d_laundromat_nearby",
                  "f_property_type*d_outdoor_dining_area",
                  "f_property_type*d_outdoor_furniture",
                  "f_property_type*d_private_entrance",
                  "f_property_type*d_smart_lock",
                  "f_property_type*d_security_cameras_on_property",
                  "f_property_type*d_have_kitchen",
                  "f_property_type*d_refrigerator",
                  "f_property_type*d_coffee_machine",
                  "f_property_type*d_wifi",
                  "f_property_type*d_bbq_equipment",
                  "f_property_type*d_tv",
                  "f_property_type*d_have_iron",
                  "f_property_type*d_have_heating",
                  "f_property_type*d_cooling",
                  "f_property_type*d_balcony",
                  "f_property_type*d_garden",
                  "f_property_type*d_have_workspace",
                  "f_property_type*d_family_friendly",
                  "f_property_type*d_bathroom_essentials",
                  "f_property_type*d_linens",
                  "f_property_type*d_body_wash")



#################################
# Create test and train samples #
#################################
# now all stuff runs on training vs test (holdout), alternative: 4-fold CV
# create test and train samples (80% of observations in train sample)
smp_size <- floor(0.8 * nrow(data))
## K = 5
k_folds <- 5
# Define seed value
seed_val <- 200
train_ids <- sample(seq_len(nrow(data)), size = smp_size)
data$train <- 0
data$train[train_ids] <- 1
# Create train and test sample variables

#data_train <- data %>% filter(train == 1)
#data_test <- data %>% filter(train == 0)                  
#  - to avoid every time this data to change when you run the entire script which was causing alot of issue for me later, I have decided to save 
# them and call them directly from GitHub

#write_csv(data_train,"Data/clean/melborne_train.csv")
#write_csv(data_test,"Data/clean/melborne_test.csv")
                  
data_train <- read_csv("https://raw.githubusercontent.com/alisial94/DA3_Airbnb_Price_Prediction/main/Data/clean/melborne_train.csv") 
data_test <- read.csv("https://raw.githubusercontent.com/alisial94/DA3_Airbnb_Price_Prediction/main/Data/clean/melborne_test.csv")
                  
                  
                  
  
#Bulding the most complex model to use in LASSO
model4 <- paste0(" ~ ",paste(c(basic_lev, reviews, host, ammenities, interactions),collapse = " + "))                
                  
# Creating the most complex OLS model to run a LASSO. Here LASSO is being used as a tool to choose predictors
# Set lasso tuning parameters:
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
formula <- formula(paste0("price ", paste(setdiff(model4, "price"), collapse = " + ")))
# Run LASSO
set.seed(seed_val)
lasso_model <- caret::train(formula,
                            data = data_train,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = train_control,
                            tuneGrid = tune_grid,
                            na.action=na.exclude)
# Check the output                   
lasso_model
# Penalty parameters
lasso_model$bestTune
# Check th optimal lambda parameter
lasso_model$bestTune$lambda
# Check the RMSE curve
plot(lasso_model)


# One can get the coefficients as well
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  # the column has a name "1", to be renamed
print(lasso_coeffs)


# Check the number of variables which actually has coefficients other than 0
lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))
print(lasso_coeffs_nz)


#write_csv(lasso_coeffs_nz,"NonZeroCoefficients.csv")

# Get the RMSE of the Lasso model 
#   Note you should compare this to the test RMSE
lasso_fitstats <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) 
lasso_fitstats

# Create an auxilary tibble
lasso_add <- tibble(Model='LASSO', Coefficients=nrow(lasso_coeffs_nz),
                    R_squared=lasso_fitstats$Rsquared, BIC = NA, 
                    Training_RMSE = NA, Test_RMSE = lasso_fitstats$RMSE )


# modifying the list of variables to be used based on LASSO results 
basic_lev <- c("f_property_type","n_accommodates","f_neighbourhood_cleansed","f_bathroom",
               "f_bedroom","d_flag_bedrooms","f_minimum_nights")
host <- c("f_host_response_time","p_host_acceptance_rate", "d_host_identity_verified",
          "d_host_is_superhost","flag_host_acceptance_rate","flag_host_response_rate","flag_host_response_time")
reviews <- c("ln_days_since_first_review","ln_number_of_reviews",
             "flag_days_since_first_review")
ammenities <- c("d_drying_rack_for_clothing","d_elevator","d_essentials","d_fire_extinguisher","d_hangers",
                "d_hot_water_kettle","d_microwave","d_outdoor_furniture","d_private_entrance",
                "d_smart_lock","d_security_cameras_on_property","d_smoke_alarm","d_have_oven",
                "d_coffee_machine","d_tv","d_have_iron", "d_balcony",
                "d_have_breakfast",  "d_paid_parking", 
                "d_have_workspace","d_have_washer","d_have_dryer", "d_streaming_services",
                "d_cutlary_glasses", "d_luggage_dropoff_allowed","d_shampoo_conditioner","d_refrigerator","d_garden"
                )
interactions <- c("f_property_type*d_bath_tub",
                  "f_property_type*d_building_staff",
                  "f_property_type*d_elevator",
                  "f_property_type*d_extra_pillows_and_blankets",
                  "f_property_type*d_fire_extinguisher",
                  #"f_property_type*d_hangers",
                  "f_property_type*d_hot_water_kettle",
                  #"f_property_type*d_laundromat_nearby",
                  #"f_property_type*d_outdoor_dining_area",
                  "f_property_type*d_outdoor_furniture",
                  "f_property_type*d_private_entrance",
                  "f_property_type*d_security_cameras_on_property",
                  "f_property_type*d_have_kitchen",
                  "f_property_type*d_refrigerator",
                  "f_property_type*d_wifi",
                  "f_property_type*d_coffee_machine",
                  "f_property_type*d_bbq_equipment",
                  "f_property_type*d_tv",
                  "f_property_type*d_balcony",
                  "f_property_type*d_garden",
                  "f_property_type*d_family_friendly",
                  "f_property_type*d_bathroom_essentials",
                  "f_property_type*d_body_wash",
                  "f_property_type*d_have_iron",
                  #"f_property_type*d_cooling",
                  "f_property_type*d_have_workspace",
                  "f_property_type*d_linens")


# Building OLS models
model1 <- " ~ n_accommodates"
model2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
model3 <- paste0(" ~ ",paste(c(basic_lev, reviews, host, ammenities),collapse = " + "))



##############################
#   cross validation OLS    #
##############################


# Do the iteration
library(fixest)
for ( i in 1:4 ){
  print(paste0( "Estimating model: " ,i ))
  # Get the model name
  model_name <-  paste0("model",i)
  model_pretty_name <- paste0("M",i,"")
  # Specify the formula
  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))
  
  # Estimate model on the whole sample
  model_work_data <- feols( formula , data = data_train , vcov='hetero' )
  #  and get the summary statistics
  fs  <- fitstat(model_work_data,c('rmse','r2','bic'))
  BIC <- fs$bic
  r2  <- fs$r2
  rmse_train <- fs$rmse
  ncoeff <- length( model_work_data$coefficients )
  
  # Do the k-fold estimation
  set.seed(seed_val)
  cv_i <- train( formula, data_train, method = "lm", 
                 trControl = trainControl(method = "cv", number = k_folds))
  rmse_test <- mean( cv_i$resample$RMSE )
  
  # Save the results
  model_add <- tibble(Model=model_pretty_name, Coefficients=ncoeff,
                      R_squared=r2, BIC = BIC, 
                      Training_RMSE = rmse_train, Test_RMSE = rmse_test )
  if ( i == 1 ){
    model_results <- model_add
  } else{
    model_results <- rbind( model_results , model_add )
  }
}

# Check summary table
# Add it to final results

model_results <- rbind( model_results , lasso_add )
model_results

# Based on the results, model4 is clearly over fitted. The table shows that  R-squared comes out to be 1 with a 
# negative BIC. Model4 was primarily used as a model to be used in LASSO to identify 
# predictors with non-zero coefficients. Therefor, I decided to add all relevant variables.

#predictors_model3 <- c(basic_lev, reviews, host, ammenities)
predictors_model2 <- c(basic_lev)


set.seed(200)
system.time({
  ols_model <- train(
    formula(paste0("price ~ ", paste0(predictors_model2, collapse = " + "))),
    data = data_train,
    method = "lm",
    trControl = train_control
  )
})
ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))



##################
## Random Forest##
##################

predictors <- c(basic_lev, host, reviews, ammenities, interactions)
# set tuning 
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)
set.seed(1200)
system.time({
  rf_model <- train(
    formula(paste0(" price ~ ", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity",
    .num.trees=500
  )
})
rf_model
# auto tuning random forest 
set.seed(1200)
system.time({
  rf_model_auto <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    importance = "impurity",
    .num.trees=500
  )
})
rf_model_auto

##############################
# Variable Importance Plots  #
##############################


rf_model_var_imp <- ranger::importance(rf_model$finalModel)/1000
rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Borough:", varname) ) %>%
  mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))
rf_model_var_imp_df

# to have a quick look
plot(varImp(rf_model))

# have a version with top 10 vars only
var_imp<- ggplot(rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_classic()


##############################
# 2) varimp plot grouped
##############################
# grouped variable importance - keep binaries created off factors together

varnames <- rf_model$finalModel$xNames


f_neighbourhood_cleansed <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_host_varnames <- grep(("host"),varnames, value = TRUE)
#f_host_varnames <- f_host_varnames[-(9:10)] #(removing the host related flag variables)
f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)
f_reviews_varnames <- grep("review",varnames, value = TRUE)

#amenities_varnames <- c("d_bath_tub", "d_cleaning_products","d_elevator", "d_fire_extinguisher","d_hangers",
 #                       "d_hot_water", "d_hot_water_kettle", "d_microwave","d_outdoor_furniture", 
  #                      "d_private_entrance","d_smart_lock","d_security_cameras_on_property","d_have_pool", 
   #                     "d_smoke_alarm", "d_have_oven", "d_coffee_machine","d_wifi" ,"d_tv","d_have_iron",
    #                    "d_have_heating", "d_cooling", "d_balcony", "d_have_breakfast", "d_have_workspace",                                                
     #                   "d_luggage_dropoff_allowed", "d_free_parking", "d_paid_parking" , "d_streaming_services",                                            
      #                  "d_shampoo_conditioner", "d_have_washer" ,"d_have_dryer" ,"d_cutlary_glasses",
       #                 "d_building_staff", "d_laundromat_nearby", "d_outdoor_dining_area","d_have_kitchen",                                                  
        #                "d_refrigerator","d_bbq_equipment", "d_garden", "d_family_friendly","d_bathroom_essentials",                                           
         #               "d_body_wash")

amenities_varnames <- c("d_bath_tub", "d_building_staff","d_carbon_monoxide_alarm",  "d_cleaning_products", 
                        "d_dining_table", "d_drying_rack_for_clothing", "d_elevator", "d_essentials","d_extra_pillows_and_blankets",                                    
                        "d_fire_extinguisher", "d_have_first_aid","d_hangers","d_hot_water","d_hot_water_kettle", "d_laundromat_nearby",                                             
                        "d_microwave","d_outdoor_dining_area","d_outdoor_furniture" , "d_private_entrance" ,  "d_roomdarkening_shades",                                          
                        "d_smart_lock", "d_security_cameras_on_property", "d_have_pool","d_smoke_alarm",  "d_have_kitchen", "d_have_stove",                                                    
                        "d_have_oven","d_refrigerator" ,  "d_coffee_machine", "d_wifi",  "d_bbq_equipment", "d_tv", "d_have_iron",                                                     
                        "d_have_heating", "d_cooling","d_balcony","d_garden","d_have_breakfast","d_have_workspace" ,"d_family_friendly",                                               
                        "d_luggage_dropoff_allowed","d_single_level_home","d_bathroom_essentials","d_free_parking", "d_paid_parking",                                                  
                        "d_linens","d_streaming_services", "d_shampoo_conditioner", "d_body_wash", "d_have_washer",  "d_have_dryer",                                                    
                        "d_clothing_storage", "d_cutlary_glasses")


groups <- list(Neighbourhood = f_neighbourhood_cleansed,
               Host_Related=f_host_varnames,
               Property_Type = f_property_type_varnames,
               Reviews = f_reviews_varnames,
               Amenities = amenities_varnames,
               Bathrooms = "f_bathroom",
               Bedrooms = "f_bedroom",
               Minimum_Nights = "f_minimum_nights",
               Number_Accommodates = "n_accommodates")



# Need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_var_imp_grouped <- group.importance(rf_model$finalModel, groups)
rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                          imp = rf_model_var_imp_grouped[,1]) %>% 
                                      mutate(imp_percentage = imp/sum(imp))


var_imp_g<- ggplot(rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_classic()


##Variable Importance Plots rf_model_auto

rf_model_auto_var_imp <- ranger::importance(rf_model_auto$finalModel)/1000
rf_model_auto_var_imp_df <-
  data.frame(varname = names(rf_model_auto_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Neighbourhood:", varname) ) %>%
  mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))
rf_model_auto_var_imp_df

# to have a quick look

plot(varImp(rf_model_auto))

# have a version with top 10 vars only

ggplot(rf_model_auto_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_classic()



##############################
# 2) varimp plot grouped
##############################
# grouped variable importance - keep binaries created off factors together
varnames_auto <- rf_model_auto$finalModel$xNames


f_neighbourhood_cleansed_auto <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_host_varnames_auto <- grep(("host"),varnames, value = TRUE)
f_property_type_varnames_auto <- grep("f_property_type",varnames, value = TRUE)
f_reviews_varnames_auto <- grep("review",varnames, value = TRUE)
amenities_varnames_auto <- c("d_bath_tub", "d_building_staff","d_carbon_monoxide_alarm",  "d_cleaning_products", 
                             "d_dining_table", "d_drying_rack_for_clothing", "d_elevator", "d_essentials","d_extra_pillows_and_blankets",                                    
                             "d_fire_extinguisher", "d_have_first_aid","d_hangers","d_hot_water","d_hot_water_kettle", "d_laundromat_nearby",                                             
                             "d_microwave","d_outdoor_dining_area","d_outdoor_furniture" , "d_private_entrance" ,  "d_roomdarkening_shades",                                          
                             "d_smart_lock", "d_security_cameras_on_property", "d_have_pool","d_smoke_alarm",  "d_have_kitchen", "d_have_stove",                                                    
                             "d_have_oven","d_refrigerator" ,  "d_coffee_machine", "d_wifi",  "d_bbq_equipment", "d_tv", "d_have_iron",                                                     
                             "d_have_heating", "d_cooling","d_balcony","d_garden","d_have_breakfast","d_have_workspace" ,"d_family_friendly",                                               
                             "d_luggage_dropoff_allowed","d_single_level_home","d_bathroom_essentials","d_free_parking", "d_paid_parking",                                                  
                             "d_linens","d_streaming_services", "d_shampoo_conditioner", "d_body_wash", "d_have_washer",  "d_have_dryer",                                                    
                             "d_clothing_storage", "d_cutlary_glasses")

groups_auto <- list(Neighbourhood = f_neighbourhood_cleansed,
               Host_Related=f_host_varnames,
               Property_Type = f_property_type_varnames,
               Reviews = f_reviews_varnames,
               Amenities = amenities_varnames,
               Bathrooms = "f_bathroom",
               Bedrooms = "f_bedroom",
               Minimum_Nights = "f_minimum_nights",
               Number_Accommodates = "n_accommodates")



# Need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups_auto) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}
rf_model_auto_var_imp_grouped <- group.importance(rf_model_auto$finalModel, groups)
rf_model_auto_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_auto_var_imp_grouped),
                                               imp = rf_model_auto_var_imp_grouped[,1])  %>%
                                                mutate(imp_percentage = imp/sum(imp))
ggplot(rf_model_auto_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_classic()


# evaluate random forests 
results <- resamples(
  list(
    model_1  = rf_model,
    model_auto  = rf_model_auto
  )
)
summary(results)


# CART with pruning
# CART with built-in pruning
set.seed(1335)
system.time({
  cart_model <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "rpart",
    tuneLength = 10,
    trControl = train_control
  )
})
cart_model

library(rpart)
library(rpart.plot)

# Tree graph
rpart.plot(cart_model$finalModel, tweak=1.2, digits=-1, extra=1)


# GBM
gbm_grid <-  expand.grid(interaction.depth = 5, # complexity of the tree
                         n.trees = 250, # number of iterations, i.e. trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)
set.seed(109)
system.time({
  gbm_model <- train(formula(paste0("price ~", paste0(predictors, collapse = " + "))),
                     data = data_train,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})
gbm_model
gbm_model$finalModel



# get prediction rmse and add to next summary table
# ---- compare these models

final_models <-
  list("OLS" = ols_model,
       "CART" = cart_model,
       "Random forest 1: Tuning provided" = rf_model,
       "Random forest 2: Auto Tuning" = rf_model_auto,
       "GBM"  = gbm_model)
results <- resamples(final_models) %>% summary()
results

# Evaluating both data sets
result_r2 <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~Rsquared")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV Rsquared" = ".")

result_r2

# Model selection is carried out on this CV RMSE

result_rmse <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")
result_rmse


# evaluate preferred model on the holdout set -----------------------------
result_2 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_test), data_test[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")
result_2

final_result4_5 <- cbind(result_r2, result_rmse, result_2)

f_r<- final_result4_5 %>%
  kbl(caption = "Horse Race of Models CV RSME", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center")


#########################################################################################
# Partial Dependence Plots for the best model; random forest  with auto tuning parameters
#########################################################################################

# 1) Property Type
pdp_f_property_type <- pdp::partial(rf_model, pred.var = "f_property_type", 
                                    pred.grid = distinct_(data_test, "f_property_type"), 
                                    train = data_train)
pdp_f_property_type %>%
  autoplot( ) +
  geom_point(color='red', size=2) +
  geom_line(color='red', size=1) +
  ylab("Predicted price") +
  xlab("Property Type") +
  theme_classic()


# 2) Number of accommodates
pdp_n_accommodates <- pdp::partial(rf_model, pred.var = "n_accommodates", 
                                   pred.grid = distinct_(data_test, "n_accommodates"), 
                                   train = data_train)
pdp_a<- pdp_n_accommodates %>%
  autoplot( ) +
  geom_point(color='red', size=4) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  theme_classic()

# 3) Bedrooms
pdp_f_bedrooms <- pdp::partial(rf_model, pred.var = "f_bedroom", 
                                   pred.grid = distinct_(data_test, "f_bedroom"), 
                                   train = data_train)
pdp_f<- pdp_f_bedrooms %>%
  autoplot( ) +
  geom_point(color='red', size=4) +
  ylab("Predicted price") +
  xlab("Number of Bedrooms") +
  theme_classic()






```

## Introduction

The aim of this report is build price prediction models, which will help a company that operates **small and mid-size apartments hosting 2-6 guests in Melbourne, Australia**. The predicted prices produced as a result of the analysis explained below will form the base for the company to price their new apartments that are going on market soon. The data used for prediction is the **Airbnb prices in Melbourne,Australia.** obtained from **[Inside Airbnb](http://insideairbnb.com/get-the-data.html)**. The prediction models were built on various predictors, such as the type of property and people it accommodates, locality or what unique features it includes, also incorporating the information about the host and reviews. The prediction algorithms used for this analysis are OLS, Cart, Random forest (with and without tuning) and GBM. I also used LASSO, but as a predictor to extract important variables to be used in the models. After conducting the analysis, the model choose for final prediction based on its performance is **Random Forest with Tuning Parameters**. The entirety of this project, including codes and data, is available on my **[GitHub](https://github.com/alisial94/DA3_Airbnb_Price_Prediction)** (please click to open the link).


## The Data and Cleaning 

The raw data is available on Inside Airbnb, for convenience, It has also been uploaded to my GitHub and can be directly retrieved from there (raw data can be found **[here](https://github.com/alisial94/DA3_Airbnb_Price_Prediction/blob/main/Data/raw/raw_melb_listing.csv)**).The data is of cross-sectional nature containing information related Airbnb listings in Melbourne, Australia which was scrapped between January 08, 2022 and January 09, 2022. Raw data includes 17409 observations (unique listings) and 74 variables. The codes for adjusted raw data explained are available on my **[GitHub](https://github.com/alisial94/DA3_Airbnb_Price_Prediction/blob/main/Codes/1_data_cleaning.R)** (please click to open the link). 

I begin cleaning the data by removing all unnecessary variables and also made alterations to the information recorded in the variables to make them useful for analysis. As we predicting the property rental prices so price is our target variable which was recorded in local price and had to be converted to USD. The major task involved in the basic cleaning  was to address the amenities. All the amenities were record in the same column as a string and I decide to split theses into individual columns.The new created amenities column amounted to 1468, which were the pooled into meaningful categories. Amenities such as TV or WiFi/internet had a lot of columns that resulted due to their unique features such as speed of internet or size of the TV. Upon creating these pooled binary categories only 79 amenities column remained. 

The data was further filter to the parameters set by the company supporting this study. Since we are only interested in apartments or housing units accommodating 2 to 6 people, therefore, I  filtered the data based on these two requirements. For property type, I selected four categories that are Condo, Serviced Apartments, Loft and Home/Apartment. Other important variables, such as number of bedrooms or review score ratings, that contained missing values were also imputed and a additional flag variable was added where missing values were more than 5%. The cleaning phase ended with variables having no missing values and **802** remaining observations **119** variables.. Codes for data preparation have been uploaded to my **[GitHub](https://github.com/alisial94/DA3_Airbnb_Price_Prediction/blob/main/Data/clean/melborne_prepared.csv)** (please click to open the link).


## Explanatory Variables

Due to the complexity the variable selection phase was a hulcurian task, eventually based on my understanding I categoriesed these varibles as follows:

- *Factor variables*: Type of property, neighborhood including flag and factorised variable of size variables (such as number of accommodates/bedrooms/beds/baths/minimum nights).

- *Reviews variables*: Review score rating, total number of reviews, total reviews for the property every month, number of days since first review (etc.) including flags.

- *Host variables*: Dummies created for host verification, host being a super host or not, host response and acceptance rates (etc.) including flags.

- *Dummy Amenities*: this included all the binary variables created for amenities being offered at the property.


## Exploratory Data Analysis
Upon completion of data cleaning, grouping and feature engineering, I examined the distribution of our target variable i.e. **Price**. The price data, as always, had a right long tail and I felt it was wise to drop few large values. Therefor I filtered the data to observations that had less than USD 400 price. I also decided to use price as is since interpertaion with log normal distribution is complex. Below you can see the graph for price and price based on property type. The interaction terms were also added for the amenities and other binary variables to the models based on their relationship with the property type and impact on prices.


```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 2, fig.align="center",message=FALSE}
price_distribution <-ggplot(data = data, aes(x = price)) +
  geom_histogram( fill='navyblue', color = 'white' ) +
  labs(x = "Price (USD)",y = "Count", title = "Price Distribution") +
  theme_classic()

price_vs_property_box <- ggplot(data=data, aes(x = f_property_type, y = price)) +
  stat_boxplot(aes(group = f_property_type), geom = "errorbar", width = 0.3,
               color = c(color[2],color[1],color[3],color[4]), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_property_type),
               color = c(color[2],color[1],color[3],color[4]), fill = c(color[2],color[1],color[3],color[4]),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,350), breaks = seq(0,350,50)) +
  labs(x = "Property type",y = "Price (USD)")+
  theme_classic()+
  ggtitle('Distribution of prices by Property type')


ggarrange(price_distribution, price_vs_property_box,
           hjust = -0.6,
            ncol = 2, nrow = 1)


```


## Prediction Modeling 
After obtaining the data set that was fit for regression analysis, the data was further split in to train and test smaples. Since I had limited data to work with, to maximise the performance, I divided the observations by adding 70% to train and remaining to test. These samples were used in the machine learning models which are OLS, Random Forest with and without tuning, CART and GBM. 

OLS models were built using LASSO as a predictor. Initially I built the most complex model (Model 4) which included the maximum number of explanatory variables and interaction terms. I used this over-fitted model to run LASSO which helped in identifying the key predictors and interaction terms (variables with non zero coefficients). 

Based on the results produced from 5-fold cross validated Root Mean Squared Error, prediction model random forest with tuning parameters performed the best. Therefore, I decided to select that model further validation and testing. For tuned FR, the maximum number of trees was set at 500, the lowest RMSE value was 48.05 which was obtained with 5 terminal nodes and 12 variables in each node. 

**Variable Importance:** The purpose of variable importance is to identify the predictors that impact the target variable the most. In this study we used the best performing model which is Random Forest with tuning to identify these variables. Number of bedrooms, number of people the property accommodates and number of bathrooms were the top performing variables. Below you can also see the graph highlighting the top 10 variables. Moreover, the second graph show the grouped explanatory variables importance graph which clearly shows that overall amenities and property type tend to be the most impact on the price. 

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 2, fig.align="center",message=FALSE}

ggarrange(var_imp, var_imp_g,
           hjust = -0.6,
            ncol = 2, nrow = 1)

```


**Partial Dependencies and Sub Sample:** Based on the results from the variable importance plot we analyse the shape of association between average y and important x variables, condition on the rest (BEKES, 2022). To do this I decided to take two most important variables: number of people accommodated by the property and  number of bedrooms. Below you can see the partial dependency plots for these variables. For both variables the PDP rather shows a fairly linear relationship with predicted prices. Using the Sub Sample approach, I predicted prices using important x variables. Based on the result of sub sample, we can say the company should invest in serviced apartments because it had the lowest error and highest predicted price i.e. $149.32. 

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 2, fig.align="center",message=FALSE}

ggarrange(pdp_a, pdp_f,
           hjust = -0.6,
            ncol = 2, nrow = 1)
```



## Conclusion

Analysis performed for predicting price results in Random Forest model with tuning performed the best since it produces the lowest RMSE value. However, due to the limitation of Random Forest because it is considered as a black-box model, therefore, it is usually difficult to explain the regressions to anyone not familiar with it. Even though, to avoid complexity, I recommend to rather select an OLS model, which in this case was model 3. The RMSE difference between Random Forest Auto Tune and OLS (model 2 since that was selected for comparison) was approximately $10 which is alot to be disregarded. Thus, I will stick with selecting random forest as the preferred model. Details other algorithm (CART and GBM), please refer to the technical report. 
